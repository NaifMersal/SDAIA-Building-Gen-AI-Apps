{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Building the Brain\n",
    "\n",
    "## From Scratch to Framework\n",
    "\n",
    "In this lab, you will:\n",
    "\n",
    "1. **Part 1:** Build a \"raw\" ReAct agent from scratch using a simple `while` loop and text parsing\n",
    "2. **Part 2:** Compare your raw agent with the project's `ReactAgent` that uses native function calling\n",
    "\n",
    "### Learning Goals\n",
    "- Understand that an agent is just a **loop** with state and reasoning\n",
    "- Experience the **pain** of parsing free-text tool calls\n",
    "- Appreciate why **native function calling** is used in production\n",
    "\n",
    "### Prerequisites\n",
    "- `uv pip install litellm python-dotenv`\n",
    "- A valid API key in your `.env` file (e.g., `OPENAI_API_KEY=sk-...`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from litellm import completion\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = os.getenv(\"MODEL_NAME\", \"gpt-4o\")\n",
    "print(f\"Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: The \"Raw\" ReAct Agent\n",
    "\n",
    "We'll build a ReAct agent that uses **text-based** tool calling. The LLM outputs plain text in a specific format, and we parse it to extract tool calls.\n",
    "\n",
    "### Step 1: Define Mock Tools\n",
    "\n",
    "First, let's create some simple tools our agent can use. These are just Python functions \u2014 no API keys needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool registry \u2014 maps tool names to functions\n",
    "TOOLS = {\n",
    "    \"search\": search,\n",
    "    \"calculate\": calculate,\n",
    "}\n",
    "\n",
    "# Define tools as OpenAI-compatible schemas for native calling\n",
    "TOOLS_SCHEMA = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search\",\n",
    "            \"description\": \"Search for information on a topic. Returns relevant text.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate\",\n",
    "            \"description\": \"Evaluate a mathematical expression. Returns the result.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"expression\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The math expression to evaluate (e.g., '15 * 500 / 100')\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"expression\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 2: The ReAct System Prompt\n\nDefine a system prompt that instructs the LLM to follow the Thought -> Action -> Observation format **exactly**."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REACT_SYSTEM_PROMPT = \"\"\"You are a helpful research assistant that solves tasks step by step.\n",
    "\n",
    "You have access to these tools:\n",
    "- search(query): Search for information. Input: a search query string.\n",
    "- calculate(expression): Evaluate a math expression. Input: a math expression string.\n",
    "\n",
    "Follow this EXACT format for EVERY step:\n",
    "\n",
    "Thought: <your reasoning about what to do next>\n",
    "Action: <tool_name>(\"<argument>\")\n",
    "\n",
    "After receiving an Observation, continue with the next Thought.\n",
    "\n",
    "When you have enough information to answer, use:\n",
    "\n",
    "Thought: I have enough information to answer.\n",
    "Final Answer: <your complete answer>\n",
    "\n",
    "IMPORTANT:\n",
    "- ALWAYS start with a Thought before any Action.\n",
    "- Use EXACTLY one Action per step.\n",
    "- Wait for the Observation before your next Thought.\n",
    "- Never fabricate Observations \u2014 only use real tool results.\n",
    "\"\"\"\n",
    "\n",
    "print(\"System prompt loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 3: Build the Parser\n\n**TODO:** Complete the `parse_action` function to extract the tool name and argument from the LLM's response."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse the LLM's text response to extract a Final Answer.\n",
    "    Native tool calls are handled separately in the agent loop.\n",
    "    \n",
    "    Returns:\n",
    "        {\"type\": \"final_answer\", \"content\": \"...\"}\n",
    "        or\n",
    "        {\"type\": \"none\"}\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return {\"type\": \"none\"}\n",
    "\n",
    "    # Check for Final Answer\n",
    "    final_match = re.search(r'Final Answer:\\s*(.+)', text, re.DOTALL)\n",
    "    if final_match:\n",
    "        return {\"type\": \"final_answer\", \"content\": final_match.group(1).strip()}\n",
    "    \n",
    "    return {\"type\": \"none\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 4: Build the Agent Loop\n\n**TODO:** Complete the agent loop by:\n1. Calling the LLM with `completion()`\n2. Executing the tool when an action is parsed\n3. Appending the observation back to messages"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_react_agent(query: str, max_steps: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Run a ReAct agent loop using native tool calling.\n",
    "    \n",
    "    Returns:\n",
    "        {\"answer\": str, \"steps\": list, \"total_steps\": int}\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": REACT_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    steps = []\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Step {step + 1}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Call the LLM using litellm's completion() with native tools\n",
    "        response = completion(\n",
    "            model=MODEL, \n",
    "            messages=messages, \n",
    "            tools=TOOLS_SCHEMA,\n",
    "            tool_choice=\"auto\",\n",
    "            max_tokens=512\n",
    "        )\n",
    "        \n",
    "        message = response.choices[0].message\n",
    "        assistant_text = message.content or \"\"\n",
    "        tool_calls = message.tool_calls\n",
    "        \n",
    "        print(f\"LLM Output:\\n{assistant_text}\")\n",
    "        if tool_calls:\n",
    "            for tc in tool_calls:\n",
    "                 print(f\"Tool Call: {tc.function.name}(\\\"{tc.function.arguments}\\\")\")\n",
    "        \n",
    "        # Append assistant response (the whole message object for tool_calls) to messages\n",
    "        messages.append(message)\n",
    "        \n",
    "        # Parse for final answer in text\n",
    "        parsed = parse_response(assistant_text)\n",
    "        steps.append({\"step\": step + 1, \"raw_output\": assistant_text, \"tool_calls\": tool_calls, \"parsed\": parsed})\n",
    "        \n",
    "        if parsed[\"type\"] == \"final_answer\":\n",
    "            print(f\"\\nFinal Answer: {parsed['content']}\")\n",
    "            return {\"answer\": parsed[\"content\"], \"steps\": steps, \"total_steps\": step + 1}\n",
    "        \n",
    "        # Handle native tool calls\n",
    "        if tool_calls:\n",
    "            for tc in tool_calls:\n",
    "                tool_name = tc.function.name\n",
    "                arguments = json.loads(tc.function.arguments)\n",
    "                \n",
    "                # Execute the tool\n",
    "                if tool_name in TOOLS:\n",
    "                    # Use the first argument if it's a simple string, or pass all kwargs\n",
    "                    # The schemas define 'query' or 'expression'\n",
    "                    arg_value = list(arguments.values())[0] if arguments else \"\"\n",
    "                    observation = TOOLS[tool_name](arg_value)\n",
    "                else:\n",
    "                    observation = f\"Error: Tool {tool_name} not found.\"\n",
    "                \n",
    "                print(f\"\\nObservation: {observation}\")\n",
    "                \n",
    "                # Append the tool result back to messages\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tc.id,\n",
    "                    \"name\": tool_name,\n",
    "                    \"content\": observation\n",
    "                })\n",
    "        \n",
    "        elif not assistant_text and not tool_calls:\n",
    "            # Should not happen with good models, but handle as safety\n",
    "            break\n",
    "            \n",
    "    return {\n",
    "        \"answer\": assistant_text or \"[Agent reached max steps without a final answer]\",\n",
    "        \"steps\": steps,\n",
    "        \"total_steps\": max_steps,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Test Your Agent\n",
    "\n",
    "Try these queries \u2014 they require multi-step reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Multi-step factual question\n",
    "result = run_react_agent(\"What is the population of the capital of France?\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Total steps: {result['total_steps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Requires search + calculation\n",
    "result = run_react_agent(\"How tall is the Eiffel Tower in feet? What is that height divided by 3?\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Total steps: {result['total_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection: The Pain Points\n",
    "\n",
    "After running the agent, think about these questions:\n",
    "\n",
    "1. **Did the parser always work?** Did the LLM deviate from the expected format?\n",
    "2. **How fragile is the regex?** What happens if the model writes `Action: search('query')` instead of `search(\"query\")`?\n",
    "3. **How would you handle multiple tool calls per step?** (Hint: you can't with text parsing \u2014 but native calling supports it)\n",
    "4. **How would you debug a failure?** You have the raw text, but no structured trace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 2: Native Function Calling\n\nNow examine the **native** approach. The same agent loop \u2014 but the LLM returns structured JSON instead of text you parse."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOOLS_SCHEMA was moved up for use in Part 1.\n",
    "print(\"Using TOOLS_SCHEMA defined in earlier Step.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_native_agent(query: str, max_steps: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Run an agent using native function calling.\n",
    "    No text parsing needed \u2014 the API returns structured tool calls.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful research assistant. Use the provided tools to answer questions.\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    steps = []\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        print(f\"\\n--- Step {step + 1} ---\")\n",
    "        \n",
    "        # Call LLM with tools parameter\n",
    "        response = completion(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            tools=TOOLS_SCHEMA,\n",
    "            tool_choice=\"auto\",\n",
    "            max_tokens=512,\n",
    "        )\n",
    "        \n",
    "        message = response.choices[0].message\n",
    "        assistant_content = message.content\n",
    "        tool_calls = message.tool_calls\n",
    "        \n",
    "        # Add assistant message to history\n",
    "        messages.append(message)\n",
    "        \n",
    "        step_info = {\"step\": step + 1, \"content\": assistant_content, \"tool_calls\": []}\n",
    "        \n",
    "        if assistant_content:\n",
    "            print(f\"Content: {assistant_content[:200]}\")\n",
    "        \n",
    "        # Handle tool calls \u2014 structured JSON, no parsing needed!\n",
    "        if tool_calls:\n",
    "            for tc in tool_calls:\n",
    "                func_name = tc.function.name\n",
    "                func_args = json.loads(tc.function.arguments)\n",
    "                \n",
    "                print(f\"Tool Call: {func_name}({func_args})\")\n",
    "                \n",
    "                # Execute tool\n",
    "                if func_name in TOOLS:\n",
    "                    result = TOOLS[func_name](**func_args)\n",
    "                else:\n",
    "                    result = f\"Error: Unknown tool '{func_name}'\"\n",
    "                \n",
    "                print(f\"Result: {result}\")\n",
    "                \n",
    "                step_info[\"tool_calls\"].append({\n",
    "                    \"tool\": func_name, \"args\": func_args, \"result\": result\n",
    "                })\n",
    "                \n",
    "                # Feed result back \u2014 structured format\n",
    "                messages.append({\n",
    "                    \"tool_call_id\": tc.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": func_name,\n",
    "                    \"content\": result,\n",
    "                })\n",
    "        \n",
    "        steps.append(step_info)\n",
    "        \n",
    "        # If no tool calls and we have content, the agent is done\n",
    "        if not tool_calls and assistant_content:\n",
    "            return {\"answer\": assistant_content, \"steps\": steps, \"total_steps\": step + 1}\n",
    "    \n",
    "    return {\n",
    "        \"answer\": \"[Max steps reached]\",\n",
    "        \"steps\": steps,\n",
    "        \"total_steps\": max_steps,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the native agent with the same query\n",
    "result = run_native_agent(\"What is the population of the capital of France?\")\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Total steps: {result['total_steps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Part 3: Compare and Reflect\n\n### Side-by-Side Comparison\n\nRun both agents on the same query and compare:\n\n| Metric | Text-Based | Native |\n|--------|-----------|--------|\n| Steps taken | ? | ? |\n| Parse errors | ? | 0 (guaranteed) |\n| Code complexity | High (regex) | Low (structured) |\n| Debugging ease | Hard | Easy |\n\n### Next Steps\n\nOpen the project's `src/agent/react_agent.py` and compare with your native agent above. Note how it:\n- Uses a `ToolRegistry` for dynamic tool management\n- Implements proper error handling with try/except\n- Logs each step for debugging (preview of Session 3's tracing)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}