{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 2: API Client Integration\n",
                "\n",
                "In this lab you will build a production-ready LLM API client using **LiteLLM** and **OpenRouter**.\n",
                "\n",
                "**By the end of this lab you will know how to:**\n",
                "- Securely load API keys from environment variables\n",
                "- Make your first LLM API call via LiteLLM\n",
                "- Use LiteLLM's built-in retry logic\n",
                "- Use LiteLLM's built-in response caching"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1 — Environment Setup\n",
                "\n",
                "We never hardcode API keys in source code. Instead, we load them from a `.env` file at runtime.\n",
                "\n",
                "Create a `.env` file in this directory with:\n",
                "```\n",
                "OPENROUTER_API_KEY=sk-or-...\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()  # Reads .env from the current directory\n",
                "\n",
                "\n",
                "def get_api_key() -> str:\n",
                "    \"\"\"Retrieve and validate the OpenRouter API key.\"\"\"\n",
                "    token = os.getenv(\"OPENROUTER_API_KEY\")\n",
                "    if not token:\n",
                "        raise EnvironmentError(\n",
                "            \"OPENROUTER_API_KEY not found. \"\n",
                "            \"Create a .env file with your key or set the environment variable.\"\n",
                "        )\n",
                "    return token\n",
                "\n",
                "\n",
                "get_api_key()  # Validate early — fail fast if the key is missing\n",
                "print(\"API key loaded successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2 — Your First API Call\n",
                "\n",
                "[LiteLLM](https://docs.litellm.ai/docs/) provides a unified `completion()` interface across 100+ LLM providers.\n",
                "We prefix the model name with `openrouter/` so LiteLLM knows to route through OpenRouter.\n",
                "\n",
                "Key features:\n",
                "\n",
                "- Direct Python library integration in your codebase\n",
                "- Router with retry/fallback logic across multiple deployments (e.g. Azure/OpenAI) - Router\n",
                "- Application-level load balancing and cost tracking\n",
                "- Exception handling with OpenAI-compatible errors\n",
                "- Observability callbacks (Lunary, MLflow, Langfuse, etc.)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from litellm import completion\n",
                "\n",
                "MODEL_ID = \"openrouter/meta-llama/llama-3-8b-instruct:free\"\n",
                "\n",
                "prompt = \"Explain what a vector database is in one paragraph.\"\n",
                "\n",
                "response = completion(\n",
                "    model=MODEL_ID,\n",
                "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "    max_tokens=150,\n",
                "    temperature=0.7,\n",
                ")\n",
                "\n",
                "print(response.choices[0].message.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3 — Retry Logic\n",
                "\n",
                "Free-tier APIs are rate-limited. Instead of writing manual retry loops, LiteLLM has a built-in `num_retries` parameter that automatically retries on `RateLimitError` and network errors with exponential backoff."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "response = completion(\n",
                "    model=MODEL_ID,\n",
                "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
                "    max_tokens=150,\n",
                "    temperature=0.7,\n",
                "    num_retries=3,   # LiteLLM retries automatically on rate limit / network errors\n",
                "    timeout=120,\n",
                ")\n",
                "\n",
                "print(response.choices[0].message.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4 — Response Caching\n",
                "\n",
                "During development, you often run the same queries many times. Caching avoids redundant API calls, saving both time and quota.\n",
                "\n",
                "LiteLLM ships with built-in caching — one line to enable it globally."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import litellm\n",
                "\n",
                "# Enable in-memory caching for all subsequent completion() calls\n",
                "litellm.enable_cache(type=\"local\")\n",
                "\n",
                "model = \"openrouter/meta-llama/llama-3-8b-instruct:free\"\n",
                "messages = [{\"role\": \"user\", \"content\": \"What is retrieval-augmented generation?\"}]\n",
                "\n",
                "print(\"--- First call (Cache MISS — hits API) ---\")\n",
                "result1 = completion(model=model, messages=messages, num_retries=3, timeout=120, max_tokens=200)\n",
                "print(result1.choices[0].message.content[:200])\n",
                "\n",
                "print(\"\\n--- Second call (Cache HIT — served from memory) ---\")\n",
                "result2 = completion(model=model, messages=messages, num_retries=3, timeout=120, max_tokens=200)\n",
                "print(result2.choices[0].message.content[:200])\n",
                "\n",
                "print(\"\\nResponses identical:\", result1.choices[0].message.content == result2.choices[0].message.content)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
