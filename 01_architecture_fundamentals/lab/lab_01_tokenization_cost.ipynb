{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab 1: Tokenization & Cost Analysis\n",
                "\n",
                "Three ideas to take away:\n",
                "1. **Tokens ≠ words** — LLMs see subword units, and counts vary by text type\n",
                "2. **Tokens cost money** — pricing differs dramatically across models\n",
                "3. **Context windows scale badly** — attention is O(n²)\n",
                "\n",
                "Run every cell top-to-bottom and observe the outputs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!uv pip install tiktoken plotly -q"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 1: Tokens ≠ Words\n",
                "\n",
                "The tokenizer breaks text into **subword units** — not characters, not whole words.\n",
                "Watch how the same encoder splits English prose, code, and Arabic differently."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tiktoken\n",
                "\n",
                "encoder = tiktoken.encoding_for_model(\"gpt-4\")\n",
                "\n",
                "samples = {\n",
                "    \"English prose\": \"The transformer architecture revolutionized natural language processing.\",\n",
                "    \"Python code\":   \"def calculate_sum(a, b):\\n    return a + b\",\n",
                "    \"Multilingual\":  \"Bonjour! 你好! Today we're discussing tokens.\",\n",
                "    \"Arabic\":        \"اسم المستخدم أحمد وعمره 30 سنة ويسكن في الرياض.\",\n",
                "}\n",
                "\n",
                "print(f\"{'Sample':<20} {'Chars':>6} {'Tokens':>7} {'Ratio':>7}   Breakdown\")\n",
                "print(\"-\" * 80)\n",
                "for name, text in samples.items():\n",
                "    tokens = encoder.encode(text)\n",
                "    pieces = [encoder.decode([t]) for t in tokens]\n",
                "    ratio  = len(tokens) / len(text)\n",
                "    print(f\"{name:<20} {len(text):>6} {len(tokens):>7} {ratio:>7.2f}   {pieces}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Key observation:** Arabic and multilingual text produce far more tokens per character than plain English — because BPE was trained mostly on English text."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 2: Tokens Cost Money\n",
                "\n",
                "Every token in and out costs real money. Let's see what processing a 10-page document costs across different models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pricing reference (USD per 1M tokens, approximate early 2025)\n",
                "PRICING = {\n",
                "    \"GPT-4-Turbo\":      {\"input\": 10.00, \"output\": 30.00},\n",
                "    \"GPT-4o\":           {\"input\":  2.50, \"output\": 10.00},\n",
                "    \"GPT-3.5-Turbo\":    {\"input\":  0.50, \"output\":  1.50},\n",
                "    \"Claude-3.5-Sonnet\":{\"input\":  3.00, \"output\": 15.00},\n",
                "    \"Gemini-1.5-Pro\":   {\"input\":  1.25, \"output\":  5.00},\n",
                "}\n",
                "\n",
                "def estimate_cost(text, model_name, expected_output_tokens=500):\n",
                "    enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
                "    input_tokens = len(enc.encode(text))\n",
                "    prices = PRICING[model_name]\n",
                "    input_cost  = (input_tokens          / 1_000_000) * prices[\"input\"]\n",
                "    output_cost = (expected_output_tokens / 1_000_000) * prices[\"output\"]\n",
                "    return {\n",
                "        \"model\":        model_name,\n",
                "        \"input_tokens\": input_tokens,\n",
                "        \"input_cost\":   input_cost,\n",
                "        \"output_cost\":  output_cost,\n",
                "        \"total_cost\":   input_cost + output_cost,\n",
                "    }\n",
                "\n",
                "# Simulate a 10-page document (~5,000 words)\n",
                "doc = (\"The rapid advancement of artificial intelligence has transformed \"\n",
                "       \"industries across the globe. Organizations are increasingly adopting \"\n",
                "       \"machine learning systems for automation, analysis, and decision-making. \") * 170\n",
                "\n",
                "print(f\"Document: {len(doc):,} chars, ~{len(doc.split()):,} words\\n\")\n",
                "print(f\"{'Model':<22} {'Input tokens':>13} {'Input $':>9} {'Output $':>9} {'Total $':>9}\")\n",
                "print(\"-\" * 66)\n",
                "for model in PRICING:\n",
                "    r = estimate_cost(doc, model, expected_output_tokens=500)\n",
                "    print(f\"{r['model']:<22} {r['input_tokens']:>13,} {r['input_cost']:>9.4f} {r['output_cost']:>9.4f} {r['total_cost']:>9.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Key observation:** GPT-4-Turbo costs ~20× more than GPT-3.5-Turbo for the same document. Model choice matters enormously at scale."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Part 3: Context Windows & O(n²) Scaling\n",
                "\n",
                "Context windows have grown from 1K to 1M tokens in 5 years — but attention cost grows **quadratically**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import plotly.graph_objects as go\n",
                "\n",
                "models = [\n",
                "    (\"GPT-2\",         2019,    1_024),\n",
                "    (\"GPT-3\",         2020,    4_096),\n",
                "    (\"GPT-3.5\",       2022,    4_096),\n",
                "    (\"Claude 1\",      2023,    9_000),\n",
                "    (\"GPT-4\",         2023,    8_192),\n",
                "    (\"GPT-4-32K\",     2023,   32_768),\n",
                "    (\"Claude 2\",      2023,  100_000),\n",
                "    (\"GPT-4-Turbo\",   2023,  128_000),\n",
                "    (\"Claude 3.5\",    2024,  200_000),\n",
                "    (\"Gemini 1.5 Pro\",2024,1_000_000),\n",
                "]\n",
                "\n",
                "names = [m[0] for m in models]\n",
                "years = [m[1] for m in models]\n",
                "sizes = [m[2] for m in models]\n",
                "\n",
                "fig = go.Figure(go.Bar(\n",
                "    x=names,\n",
                "    y=sizes,\n",
                "    marker_color=[\"#9B8EC0\" if y < 2023 else \"#00C9A7\" if y == 2023 else \"#FF7A5C\" for y in years],\n",
                "    text=[f\"{s:,}\" for s in sizes],\n",
                "    textposition=\"outside\",\n",
                "))\n",
                "fig.update_layout(\n",
                "    title=\"Context Window Evolution (log scale)\",\n",
                "    yaxis_title=\"Max Context Length (tokens)\",\n",
                "    yaxis_type=\"log\",\n",
                "    template=\"plotly_white\",\n",
                "    height=480,\n",
                ")\n",
                "fig.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# O(n²): doubling context → 4× more attention computations\n",
                "context_lengths = [1_024, 4_096, 8_192, 32_768, 128_000]\n",
                "\n",
                "print(f\"{'Tokens':>10}  {'Attention ops':>18}  {'Relative cost':>14}\")\n",
                "print(\"-\" * 48)\n",
                "base = context_lengths[0] ** 2\n",
                "for n in context_lengths:\n",
                "    ops = n ** 2\n",
                "    print(f\"{n:>10,}  {ops:>18,}  {ops/base:>13.0f}×\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Key observation:** Going from 1K → 128K tokens makes the context window **16,384× more expensive** to compute. Bigger isn't always better."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Bonus: Tokens vs Characters Across Formats\n",
                "\n",
                "The same information expressed in different formats tokenizes very differently."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "formats = {\n",
                "    \"English\": \"The user's name is Ahmed and he is 30 years old and lives in Riyadh.\",\n",
                "    \"Python\":  'user = {\"name\": \"Ahmed\", \"age\": 30, \"city\": \"Riyadh\"}',\n",
                "    \"JSON\":    '{\"name\": \"Ahmed\", \"age\": 30, \"city\": \"Riyadh\"}',\n",
                "    \"Arabic\":  \"اسم المستخدم أحمد وعمره 30 سنة ويسكن في الرياض.\",\n",
                "}\n",
                "\n",
                "labels = list(formats.keys())\n",
                "token_counts = [len(encoder.encode(t)) for t in formats.values()]\n",
                "char_counts  = [len(t) for t in formats.values()]\n",
                "\n",
                "fig = go.Figure()\n",
                "fig.add_trace(go.Bar(name=\"Tokens\",     x=labels, y=token_counts, marker_color=\"#1C355E\",\n",
                "                     text=token_counts, textposition=\"outside\"))\n",
                "fig.add_trace(go.Bar(name=\"Characters\", x=labels, y=char_counts,  marker_color=\"#00C9A7\",\n",
                "                     text=char_counts,  textposition=\"outside\"))\n",
                "fig.update_layout(\n",
                "    title=\"Token vs Character Counts Across Formats\",\n",
                "    yaxis_title=\"Count\",\n",
                "    barmode=\"group\",\n",
                "    template=\"plotly_white\",\n",
                "    height=430,\n",
                ")\n",
                "fig.show()\n",
                "\n",
                "print(\"\\nTokens per character:\")\n",
                "for label, tc, cc in zip(labels, token_counts, char_counts):\n",
                "    print(f\"  {label:<10} {tc/cc:.2f} tokens/char   ({tc} tokens, {cc} chars)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}